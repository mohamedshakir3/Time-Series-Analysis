\documentclass{article}
\usepackage[landscape]{geometry}
\usepackage{url}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath,amssymb}
\usepackage{listings}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage{environ}
\makeatletter

\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\title{}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}

\advance\topmargin-.8in
\advance\textheight3in
\advance\textwidth3in
\advance\oddsidemargin-1.5in
\advance\evensidemargin-1.5in
\parindent0pt
\parskip2pt
\newcommand{\hr}{\centerline{\rule{3.5in}{1pt}}}
%\colorbox[HTML]{e4e4e4}{\makebox[\textwidth-2\fboxsep][l]{texto}


\definecolor{blue}{HTML}{A7BED3}
\definecolor{brown}{HTML}{DAB894}
\definecolor{pink}{HTML}{FFCAAF}


\DeclareMathOperator{\Unif}{Unif}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bern}{Bernoulli}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Pois}{Poisson}
\DeclareMathOperator{\atanh}{arctanh}
\DeclareMathOperator{\arma}{ARMA}
\DeclareMathOperator{\ar}{AR}
\DeclareMathOperator{\mspe}{MSPE}
\DeclareMathOperator{\ma}{MA}


\DeclareMathOperator{\vif}{VIF}
\DeclareMathOperator{\aic}{AIC}
\DeclareMathOperator{\bic}{BIC}
\DeclareMathOperator{\sch}{Sch}
\DeclareMathOperator{\bias}{Bias}
\DeclareMathOperator{\press}{PRESS}
\DeclareMathOperator{\fin}{IN}
\DeclareMathOperator{\fout}{OUT}

\DeclareMathOperator{\odds}{odds}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dev}{Dev}




\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]





\tikzset{header/.style={path picture={
\fill[green, even odd rule, rounded corners]
(path picture bounding box.south west) rectangle (path picture bounding box.north east) 
([shift={( 2pt, 4pt)}] path picture bounding box.south west) -- 
([shift={( 2pt,-2pt)}] path picture bounding box.north west) -- 
([shift={(-2pt,-4pt)}] path picture bounding box.north east) -- 
([shift={(-6pt, 6pt)}] path picture bounding box.south east) -- cycle;
},
label={[anchor=west, fill=green]north west:\textbf{#1:}},
}} 

\tikzstyle{mybox} = [draw=black, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{fancytitle} =[fill=black, text=white, rounded corners, font=\bfseries]


\tikzstyle{bluebox} = [draw=blue, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{bluetitle} =[fill=blue, inner sep=4pt, text=white, font=\small]


\tikzstyle{brownbox} = [draw=brown, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{browntitle} =[fill=brown, inner sep=4pt, text=white, font=\small]

\tikzstyle{pinkbox} = [draw=pink, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{pinktitle} =[fill=pink, inner sep=4pt, text=white, font=\small]

\tikzstyle{redbox} = [draw=red!35, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
\tikzstyle{redtitle} =[fill=red!35, inner sep=4pt, text=white, font=\small]


\NewEnviron{brownbox}[1]{
    \begin{tikzpicture}
    \node[brownbox](box){%
    \begin{minipage}{0.9\textwidth}
    \BODY
    \end{minipage}};
    \node[browntitle, right=10pt] at (box.north west) {#1};
    \end{tikzpicture}
}

 \NewEnviron{redbox}[1]{
    \begin{tikzpicture}
    \node[redbox](box){%
    \begin{minipage}{0.9\textwidth}
    \BODY
    \end{minipage}};
    \node[redtitle, right=10pt] at (box.north west) {#1};
    \end{tikzpicture}
}
   
    

\NewEnviron{bluebox}[1]{%
\begin{tikzpicture}
    \node[bluebox](box){%
        \begin{minipage}{0.9\textwidth}
            \BODY
        \end{minipage}
    };
    
\node[bluetitle, right=10pt] at (box.north west) {#1};
\end{tikzpicture}
}

\NewEnviron{pinkbox}[1]{%
\begin{tikzpicture}
    \node[pinkbox](box){%
        \begin{minipage}{0.9\textwidth}
            \BODY
        \end{minipage}
    };
    
\node[pinktitle, right=10pt] at (box.north west) {#1};
\end{tikzpicture}
}



\NewEnviron{blackbox}[1]{%
\begin{tikzpicture}
    \node[mybox](box){%
        \begin{minipage}{0.3\textwidth}
        \raggedright
        \small{
            \BODY
        }
        \end{minipage}
    };
    
\node[fancytitle, right=10pt] at (box.north west) {#1};
\end{tikzpicture}
}


\begin{document}

% \begin{center}{\large{\textbf{MAT }}}\\
% \end{center}

\begin{multicols*}{3}
\begin{blackbox}{Model Types}
    Typical structure of a time series is 
    \[X_t = m_t + Y_t + S_t\]
    where $m_t$ is a trend, $S_t$ is a seasonal part, and $Y_t$ is a stationary part.
    \begin{itemize}[leftmargin=5pt]
        \item \textbf{White Noise:} Sequence of iid random variables $\{Z_t\}$ with mean 0 and variance $\sigma_Z^2$.
        \item \textbf{Random Walk:} $X_t = X_{t-1} + Z_t$, where $\{Z_t\}$ is white noise.
        \item \textbf{MA(q):} Let $\theta(z) = 1 + \theta_1z + \theta_2z^2 + \cdots + \theta_qz^2$, 
        \[X_t = Z_t + \theta_1Z_{t-1} + \cdots + \theta_qZ_{t-q} = \theta(B)Z_t\]
        \item \textbf{AR(p):} Let $\phi(z) = 1-\phi_1z - \phi_2z^2 - \cdots - \phi_pz^p$,
        \[X_t = \phi_1X_{t-1} + \cdots + \phi_pX_{t-p} + Z_t \implies Z_t = \phi(B)X_t\]
        \item \textbf{ARMA(p,q):} $\phi(B)X_t = \theta(B)Z_t$
        \[X_t - \phi_1X_{t-1} - \cdots - \phi_pX_{t-p} = Z_t + \theta_1Z_{t-1} + \cdots + \theta_qZ_{t-q}\]
    \end{itemize}
\end{blackbox}
\begin{blackbox}{Autocovariance Functions}
    The autocovariance function of a time series $\{X_t\}$ is 
    \[\gamma_X(r,s) = \Cov(X_r, X_s) = E[(X_r - \mu_X(r))(X_s-\mu_X(s))]\]
    \begin{bluebox}{ACF For Different Models}
        \begin{itemize}[leftmargin=5pt]
            \item \textbf{White Noise:} $\gamma_Z(t+h,t) = I(h = 0)$
            \item \textbf{Random Walk:} $\gamma_S(t,t+h) = t\sigma_Z^2$
            \item \textbf{MA(1):} $\gamma_X(t+h,t) = \begin{cases}
                \sigma_Z^2(1+\theta^2) & h = 0\\
                \theta\sigma_Z^2 & h = \pm 1\\
                0 & |h| > 1
            \end{cases}$
            \item \textbf{AR(1):} $\gamma_X(h) = \sigma_Z^2\dfrac{\phi^h}{1-\phi^2}$
            \item \textbf{ARMA(1,1):} $\gamma_X(0) = \sigma_Z^2\left(1+\dfrac{(\phi + \theta)^2}{1-\phi^2}\right)$, 
            \[\gamma(1) = \sigma_Z^2\left[(\phi+\theta) + \frac{(\phi+\theta)^2\phi}{1-\phi^2}\right]\gamma(h) = \phi^{h-1}\gamma_X(1)\]
            \raggedright
            \item \textbf{AR(2):} $\gamma_X(0) = \dfrac{\sigma_Z^2(1-\phi_2)}{(1+\phi^2)[(1-\phi_2)^2-\phi_1^2]}$, $\gamma(1) =  \dfrac{\gamma(0)\phi_1}{1-\phi_2}$, $\gamma(h) = \phi_1\gamma(h-1) + \phi_2\gamma(h-2)$
            \item \textbf{MA(q):} $\gamma_X(h) = \sigma_Z^2\sum\limits_{j=0}^{q-h}\theta_j\theta_{j+h}$
        \end{itemize}
    \end{bluebox}
    \vspace{-1.3ex}
\end{blackbox}
\begin{blackbox}{ARMA Models}
    \begin{redbox}{Solutions to ARMA Models}
        \begin{enumerate}[label=\roman*), leftmargin=7pt]
            \item A stationary solution for $\arma(p,q)$ exists whenever the autoregressive polynomial $\phi(z) = 1 - \phi_1z - \cdots - \phi_pz^p \neq 0$ for all $|z| = 1$. In other words, for $\arma$ to be stationary, if $z$ solves $\phi(z) = 0$, then $|z| \neq 1$. 
            \item A stationary and causal solution for $\arma(p,q)$ exists whenever the autoregressive polynomial $\phi(z) = 1 - \phi_1z - \cdots - \phi_pz^p \neq 0$ for all $|z| \leq 1$. In other words, for $\arma$ to be stationary and causal, if $z$ solves $\phi(z) = 0$, then $|z| > 1$.
        \end{enumerate}
    \end{redbox}
    \begin{brownbox}{Linear Processes}
        \textbf{Definition}
            Let $\{Z_t\}$ be white noise. Let $\psi_j, \ j \geq 0$, be a sequence of constants such that $\sum_{j=0}^\infty \psi_j^2 < \infty$. Then, \\[-5ex]
            \[X_t = \sum_{j=0}^\infty \psi_jZ_{t-j}\]
            is called a linear process.
    \end{brownbox}
    \textbf{Linear Representation of AR(1):} $\ar(1)$ is obtained by setting $p=1$ and $q=0$, so $Z_t = \phi(B)X_t$ where $\phi(z) = 1-\phi z$. Then, we define \\[-3ex]
    \[\chi(z) = \frac{1}{\phi(z)}\]
    This function has the power series expansion when $|\phi| < 1$, \\[-4ex]
    \[\chi(z) = \frac{1}{\phi(z)} = \sum_{j=0}^\infty \phi^jz^j\]
    Multiplying this with our equation for $Z_t$, \\[-1ex]
    \[\chi(B)\phi(B)X_t = \chi(B)Z_t \iff X_t = \chi(B)Z_t\]
    Thus, our linear representation of $\ar(1)$ is \\[-3ex]
    \[X_t = \chi(B)Z_t = \sum_{j=0}^\infty \phi^jB^jZ_t = \sum_{j=0}^\infty \phi^jZ_{t-j}\]
    \textbf{Linear Representation of ARMA(1,1):} Here we have $\phi(B)X_t = \theta(B)Z_t$
    where $\phi(z) = 1 - \phi z$ and $\theta(z) = 1 + \theta z$. Then define $\chi(z)$ the same way and multiply on both sides to get $X_t = \chi(B)\theta(B)Z_t$.\\[-4ex]
    \[X_t = \sum_{j=0}^\infty \phi^jB^j(1+\theta B)Z_t = \sum_{j=0}^\infty \phi^jZ_{t-j} + \theta\sum_{j=0}^\infty \phi^jZ_{t-(j+1)}\]
    We want to write this in the form:\\[-3ex]
    \[\sum_{j=0}^\infty \psi_jZ_{t-j} = \sum_{j=0}^\infty \phi^jZ_{t-j} + \theta\sum_{j=0}^\infty\phi^j Z_{t-j-1}\]
    \[\psi_0Z_t + \sum_{j=1}^\infty \psi_jZ_{t-j} = \phi^0Z_t + \sum_{j=1}^\infty (\phi^j + \theta \phi^{j-1})Z_{j-1}\]
    \vspace{-2ex}

    Hence $\psi_0 = 1, \ \psi_j = \phi^{j-1}(\theta + \phi), \ j \geq 1$.
\end{blackbox}
\begin{blackbox}{Partial Autocorrelation Functions}
    \textbf{Definition.} The partial autocorrelation at lag $h$ for a time series $\{X_t\}$, denoted $\alpha(h)$ is the autocorrelation between $X_t$ and $X_{t+h}$ after removing the effect of the intervening values $X_{t+1}, \ldots, X_{t+h-1}$. For lag 1 we have $\alpha(1) = \rho(1)$, and lag 2\\[-3ex]
    \[\alpha(2) = \rho_{13.2} = \frac{\rho_X(2) - \rho_X^2(1)}{\sqrt{1-\rho_X^2(1)}\sqrt{1-\rho_X^2(1)}}\]
\end{blackbox}
\begin{blackbox}{Interpreting PACF/ACF Plots}
    \begin{center}
        \begin{tabular}{c|c|c}
            & $\ar(p)$ & $\ma(q)$\\ 
            \hline
            & &  \\
            PACF & Significant at each lag $p$ & Tails off \\
            & Cuts off after lag $p$ & (Geometric Decay) \\
            \hline
            & & \\
        ACF & Tails off & Significant at lag $q$ \\
        &  (Geometric Decay) & Cuts off after lag $q$ \\
        \end{tabular}
    \end{center}
    \begin{itemize}[leftmargin=5pt]
        \item Look at PACF plot first, if you have only $p$ significant lags, then you have a pure $\ar(p)$ model. No need to look at ACF. If there is oscillating behavior for significant lags, indicates presence of MA part. 
        \item If there was an $\ma(q)$ part, then look at ACF. If ACF becomes insignificant after lag $q$, then it is a pure $\ma(q)$ model. If it is significant past lag $q$, then it is a mixed $\arma$ model. No way to guess order of $\ar$ part, pick small $p \leq q$.
    \end{itemize}
\end{blackbox}

\begin{blackbox}{Yule Walker Equations}
    Denote $P_{n}X_{n+k}$ as the predicted value of $X_{n+k}$ given that we have $n$ observations.\\[-3ex] 
    \[P_nX_{n+k} = a_0 + a_1X_{n} + a_2X_{n-1} + \cdots a_nX_1\]  
    To find coefficients $a_0,\ldots, a_n$, solve the system \\[-4ex]
    \[\Gamma_n\textbf{a}_n = \gamma(n;k), \ \gamma(n;k) = \left(\gamma_X(k), \ldots, \gamma_X(k+n-1)\right)\]
    \vspace{-7ex}

    \[\Gamma_n = \left[\gamma_X(i-j)\right]^n_{i,j=1}, \ \textbf{a}_n = (a_1, \ldots, a_n)^T\]   
    The mean squared prediction error is \\[-4ex]
    \begin{align*}
        \mspe_n(k) &= E\left[\left(X_{n+k} - \sum_{i=1}^na_iX_{ n+1-    i}\right)^2\right]\\
        &= \gamma(0) - \textbf{a}_n\gamma(n;k)  
    \end{align*}
\end{blackbox}

\begin{blackbox}{Durbin-Levinson Algorithm}
    \[P_nX_{n+1} = \phi_{n1} X_n + \cdots + \phi_{nn}X_1\]
    \[\phi_{nn} = \left[\gamma_X(n) - \sum_{j=1}^{n-1}\phi_{n-1,j}\gamma_X(n-j)\right]v_{n-1}^{-1}\]
    \[(\phi_{n,i})_{i=1}^{n-1} = (\phi_{n-1,i})_{i=1}^{n-1} - \phi_{nn}(\phi_{n-1,i})_{i=n-1}^{1}\]
    \[v_n = v_{n-1}(1-\phi_{nn}^2), v_0 = \gamma_X(0), \phi_{11} = \rho_X(1)\]
    \vspace{-4ex}
\end{blackbox}
\begin{blackbox}{Useful Formulas and Theorems}
    \textbf{Autocovariance Using Linear Representation}\\[-2ex]
    \[\gamma_X(h) = \sigma_Z^2\sum_{j=0}^{\infty} \psi_j \psi_{j+h}\]
    \textbf{Evaluating Geometric Series}\\[-2ex]
    \[\sum_{k=0}^{\infty}ar^k = \sum_{k=1}^{\infty}ar^{k-1} = \frac{a}{1-r}, \ |r| < 1\]
\end{blackbox}

\begin{blackbox}{Yule-Walker and Durbin-Levinson for MA(1)}
    Yule-Walker procedure and Durbin-Levinson algorithm for $\ma(1)$ models $X_t = Z_t + \theta Z_{t-1}$, $\theta \in \real$, where $Z_t$ are i.i.d. random variables with mean 0 and variance $\sigma_Z^2$. 
\begin{enumerate}[label=\alph*), leftmargin=5pt]
    \item Let $n=2$. Using Yule-Walker equations to obtain coefficients $a_1,a_2$ in $P_2X_3 = a_1X_2 + a_2X_1$:\\[-2ex]
    \[\Gamma_2 = \begin{pmatrix}
        \gamma_X(0) & \gamma_X(1)\\
        \gamma_X(1) & \gamma_X(0)
    \end{pmatrix}\]
    \vspace{-6ex}

    \begin{align*}
        \Gamma_n\textbf{a}_n = \gamma(n;1) &\implies \Gamma_2\begin{pmatrix}
            a_1 \\a_2
        \end{pmatrix} = \gamma(2;1)\\
        &\implies \begin{pmatrix}
            a_1\\a_2
        \end{pmatrix} = \Gamma_n^{-1}\gamma(2;1)
    \end{align*}
    \[\Gamma_2^{-1} = \frac{1}{\gamma^2_X(0) - \gamma^2_X(1)}\begin{pmatrix}
        \gamma_X(0) & -\gamma_X(1)\\
        -\gamma_X(1) & \gamma_X(0)
    \end{pmatrix}\]
    The vector $\gamma(2;1)$ is $\gamma(2;1) = (\gamma(1), \gamma(2))^T$. We have the system of equations \\[-4ex]
    \begin{align*}
        \begin{pmatrix}
            a_1 \\ a_2
        \end{pmatrix} &= \frac{1}{\gamma^2_X(0) - \gamma^2_X(1)}\begin{pmatrix}
            \gamma_X(0) & -\gamma_X(1)\\
            -\gamma_X(1) & \gamma_X(0)
        \end{pmatrix}\begin{pmatrix}
            \gamma_X(1)\\ \gamma_X(2)
        \end{pmatrix}\\
        &\implies \begin{cases}
            a_1 = \dfrac{\gamma_X(0)\gamma_X(1) - \gamma_X(1)\gamma_X(2)}{\gamma^2_X(0) - \gamma^2_X(1)}\\[2.5ex]
            a_2 = \dfrac{-\gamma_X(1)^2 + \gamma_X(0)\gamma_X(2)}{\gamma_X(0)^2 - \gamma_X(1)^2}
        \end{cases}
    \end{align*}
    For the $\ma(1)$ model, the autocovariance at lag 2 is 0, so $\gamma_X(2) = 0$, and we are left with\\[-2ex]
    \[a_1 = \frac{\gamma_X(0)\gamma_X(1)}{\gamma_X^2(0) - \gamma_X^2(1)},\ a_2 = -\frac{\gamma_X^2(1)}{\gamma^2_X(0) - \gamma_X^2(1)}\]
    \item Applying Durbin-Levinson algo $P_2X_3 = \phi_{21}X_2 + \phi_{22}X_1$.\\[-4ex]
    \begin{align*}
        v_1 &= v_0[1-\phi_{11}^2] = \gamma_X(0)[1-\rho_X(1)]\\
        &= \sigma_Z^2(1+\theta^2)\left(1-\frac{\theta^2}{(1+\theta^2)^2}\right)\\
        \phi_{22} &= [\gamma_X(2) - \phi_{11}\gamma_X(1)]v_1^{-1} = -v_1^{-1}\phi_{11}\gamma_X(1)\\
        &= -\theta\sigma_Z^2\frac{\theta}{1+\theta^2}\left(\frac{1+\theta^2}{\sigma_Z^2((1+\theta^2)^2-\theta^2)}\right)
    \end{align*}
    \[ \phi_{21} = \phi_{11} - \phi_{22}\phi_{11} =\frac{\theta}{1+\theta^2} + \frac{\theta^2}{(1+\theta^2)^2 - \theta^2} \frac{\theta}{1+\theta^2}
    \]
\end{enumerate}
\end{blackbox}

    
\begin{blackbox}{Yule-Walker for AR(1)}
    Yule-Walker forecasting procedure for $\ar(p)$ models. Assume that $Z_t$ are i.i.d random variables with mean 0 and variance $\sigma_Z^2$.
\begin{enumerate}[label=\alph*),leftmargin=6pt]
    \item Apply the Yule-Walker procedure to obtain $P_nX_{n+2}$ for $\ar(1)$ model. Compute $\mspe_n(2)$. Guess a general formula for $P_nX_{n+k}$. We guess the solution $\textbf{a}_n = (\phi^2, \ldots, 0)^T$, check if its valid.
    \begin{align*}
        &\gamma_X(n;2) = (\gamma_X(2), \ldots, \gamma_X(n+1))^T\\
        &=\frac{\sigma_Z^2}{1-\phi^2}(\phi^2, \ldots, \phi^{n+1})^T
    \end{align*}
    The variance covariance matrix $\Gamma_n$ is 
    \begin{align*}
        \begin{pmatrix}
            \gamma_X(0) & \gamma_X(1) & \cdots & \gamma_X(n-1)\\
            \gamma_X(1) & \gamma_X(0) & \cdots & \gamma_X(n-2)\\
            \vdots & \vdots & \ddots & \vdots\\
            \gamma_X(n-1) & \gamma_X(n-2) & \cdots & \gamma_X(0)
        \end{pmatrix}\\ = \frac{\sigma_Z^2}{1-\phi^2}\begin{pmatrix}
            1 & \phi & \cdots & \phi^{n-1}\\
            \phi & 1 & \cdots & \phi^{n-2}\\
            \vdots & \vdots & \ddots & \vdots\\
            \phi^{n-1} & \phi^{n-2} & \cdots & 1
        \end{pmatrix}
    \end{align*}
    Thus the Yule-Walker equations are
    \[
        \frac{\sigma_Z^2}{1-\phi^2}\begin{pmatrix}
            1  & \cdots & \phi^{n-1}\\
            \vdots & \vdots & \ddots & \vdots\\
            \phi^{n-1}  & \cdots & 1
        \end{pmatrix}\begin{pmatrix}
            \phi^2\\ 0\\ \vdots\\ 0
        \end{pmatrix} = 
        \frac{\sigma_Z^2}{1-\phi^2}\begin{pmatrix}
            \phi^2\\ \vdots\\ \phi^{n+1}
        \end{pmatrix}
    \]
    $\textbf{a}_n$ is a solution since we end up with the proper covariance functions. The mean squared prediction error is
    \[
        \mspe_n(2) = \gamma_X(0) - \textbf{a}_n^T\gamma(n;k) = \gamma_X(0) - \phi^2\gamma_X(2)
    \]
    
    \item Apply the Yule-Walker procedure to obtain $P_nX_{n+1}$ for $\ar(2)$ model $X_t = \phi_1X_{t-1}+\phi_2X_{t-2} +Z_t$ . Compute
    the corresponding $\mspe_n(1)$. \textbf{Solution.} Similarly, guess $\textbf{a}_n = (\phi_1, \phi_2, \ldots, 0)^T$, verify it solves $\Gamma_n\textbf a_n = \gamma(n;1)$. Then we find $P_nX_{n+1} = \phi_1X_n + \phi_2X_{n-1}$. The mean squared prediction error is
    \[\mspe_n(1) = \gamma_X(0) - \phi_1\gamma_X(1) - \phi_2\gamma_X(2) = \sigma_Z^2\]
\end{enumerate}
\end{blackbox}

\begin{blackbox}{Recursive Method for ACF}

    We start with the $\ar(2)$ model. Then multiplying both sides by $X_{t-h}$ and taking the expected value,
    \[E(X_tX_{t-h}) = \phi_1E(X_{t-1}X_{t-h}) + \phi_2E(X_{t-2}X_{t-h})\]
    This gives us the recursive formula for the covariance of $\ar(2)$ as\\[-3ex] 
    \[\gamma_X(h) = \phi_1\gamma_X(h-1) + \phi_2\gamma_X(h-2)\]
    Then compute $\gamma_X(0), \gamma_X(1)$, Note $E(X_tZ_t) = \sigma_Z^2$
    \begin{align*}
        E(X_tX_{t-1}) &= \phi_1E(X_{t-1}X_{t-1}) + \phi_2E(X_{t-2}X_{t-1})\\
        \iff& \gamma_X(1) = \phi_1\gamma_X(0) + \phi_2\gamma_X(1)\\
        E(X_tX_t) &= \phi_1E(X_tX_{t-1}) + \phi_2E(X_tX_{t-2}) + E(X_tZ_t)\\
        \gamma_X(0) &= \phi_1\gamma_X(1) +  \phi_2\gamma_X(2) + \sigma_Z^2
    \end{align*}
    Rearranging and solving this system of equations for $\gamma_X(1)$ and $\gamma_X(0)$ gives 
    \[\gamma_X(h) = \begin{cases}
        \sigma_Z^2 \frac{1-\phi_2}{(1+\phi_2)((1-\phi_2)^2 - \phi_1^2)} & h = 0\\
        \sigma_Z^2 \frac{\phi_1}{(1+\phi_2)((1-\phi_2)^2 - \phi_1^2)} & h = 1\\
        \phi_1\gamma_X(h-1) + \phi_2\gamma_X(h-2) & h \geq 2
    \end{cases}\]       
\end{blackbox}

\begin{blackbox}{Linear Representation for ARMA(1,2)}
    Here we have $\phi(B)X_t = \theta(B)Z_t$ with $\phi(z) = 1 - \phi z$, define\\[-4ex]
    \[\chi(z) = \frac{1}{\phi(z)} = \frac{1}{1-\phi z} = \sum_{j=0}^\infty \phi^jz^j\]
    Multiplying: $\chi(B)$ $\chi(B)\phi(B)X_t = \chi(B)\theta(B)Z_t$ gives\\[-5ex]
    \begin{align*}
        X_t &= \chi(z)\theta(B)Z_t = \sum_{j=0}^\theta\phi^jB^j(1+\theta_1B+\theta_2B^2)Z_t\\
        &= \sum_{j=0}^\infty\phi^jZ_{t-j} + \sum_{j=0}^\infty\theta_1\phi^jZ_{t-j-1} + \sum_{j=0}^\infty\theta_2\phi^jZ_{t-j-2}
    \end{align*}
    We want to rewrite $X_t$ in the form $\sum_{j=0}^\infty \psi_jZ_{t-j}$, so 
\[\sum_{j=0}^\infty \psi_jZ_{t-j}= \sum_{j=0}^\infty\phi^jZ_{t-j} + \sum_{j=0}^\infty\theta_1\phi^jZ_{t-j-1} + \sum_{j=0}^\infty\theta_2\phi^jZ_{t-j-2}\]
We can rewrite this as 
\begin{align*}
    \psi_0Z_t + \psi_iZ_{t-1} &+ \sum_{j=1}^\infty \psi_jZ_{t-j} = \phi^0Z_t + (\theta + \phi_1)Z_{t-1}\\
    &+ \sum_{j=2}^\infty (\phi^j + \theta_1\phi^{j-1}+\theta_2\phi^{j-2})Z_{t-j}  
\end{align*}
Hence the linear representation of $\arma(1,2)$ is 
\[\psi_0 = 1, \ \psi_1 = \phi + \theta_1, \ \psi_j = \phi^{j-2}(\phi^2 + \theta_1\phi + \theta_2), \ j \geq 2\]
\end{blackbox}


\end{multicols*}



\end{document}
    